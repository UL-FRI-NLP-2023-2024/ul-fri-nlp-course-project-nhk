%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
% 
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}
\usepackage{verbatim}

\graphicspath{{fig/}}




%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{FRI Natural language processing course 2024}

% Interim or final report
\Archive{Project report} 
%\Archive{Final report} 

% Article title
\PaperTitle{LLM Prompt Strategies for Commonsense-Reasoning Tasks} 

% Authors (student competitors) and their info
\Authors{Matic Pristavnik Vrešnjak, Mitja Kocjančič, and Songeun Hong}

% Advisors
\affiliation{\textit{Advisors: Slavko Žitnik}}

% Keywords
\Keywords{Large Language Models (LLMs), Prompt Strategies, Comparative Analysis}
\newcommand{\keywordname}{Keywords}


%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------
\Abstract{
The surge in the popularity of Large Language Models (LLMs) such as chatGPT, PaLM, and Gemini has led to their widespread adoption in both personal and commercial domains. Many of these cutting-edge models rely on the transformer architecture. With the increasing use of LLMs, there is a growing need to devise prompts that facilitate the generation of relevant and informative responses, particularly for tasks necessitating commonsense reasoning. Such tasks draw upon everyday knowledge for resolution. Consequently, various prompt strategies have emerged to enhance model performance on such tasks. In response to the expanding array of prompt strategies, this paper offers a comprehensive comparison of each approach, aiming to shed light on their effectiveness and applicability in enhancing LLM performance.
}

%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
\flushbottom 

% Print the title and abstract box
\maketitle 

% Removes page numbering from the first page
\thispagestyle{empty} 

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}

    %Matic: Mockup of the introduction. You can change any part that you don't like about this text :).
    %Matic: I need some time to think about the finish up of the introduction.
    %Matic: I will take a one last look at this but I think I sholud be fine after today
    Recent years have seen a rise in the popularity of Large language models (LLMs). LLMs such as chatGPT, PaLM, Gemini, and others have already been adopted for personal and commercial use. Many of the current state-of-the-art models are based on the transformer architecture\cite{DBLP:journals/corr/VaswaniSPUJGKP17}. Because of the wide adoption of LLMs, it has become more important to provide prompts that encourage the model to generate relevant and informative responses to tasks that require commonsense reasoning. Tasks that require commonsense reasoning are those that require everyday knowledge to solve. This has led to the development of several strategies for providing prompts that allow models to perform better on such tasks. Due to the growing number of prompt strategies, we present a comprehensive comparison of each strategy.


    %Matic: I am debating if we should split this part into two sections. One for the introduction and one for related work
    %Sienna: I think it would be good to split two sections.
    %Matic: Ok

    \section*{Related work}
    %Matic: If you two find any methods that look interesting describe them here.
    %Matic: Added a small bit about Chains of thought in DSPy since the assistant mentioned this when I was talking to him. Sorry for the late add, I remembered at the last moment.
    One strategy for providing prompts is Chain of thought (CoT)\newline\cite{wei2023chainofthought}. The core idea of CoT is that we give the model a demonstration of how to come to the solution or ask it for a step-by-step response. CoT has been proven to improve the performance of LLMs on commonsense reasoning and mathematical datasets. One limitation mentioned in the original CoT paper was that CoT only benefits large models like chatGPT and not smaller models. Frameworks like DSPy\cite{khattab2023dspy} have also integrated functionality that enables the implementation of custom Chains of thought. Strategies with similar approaches are In-context Learning\cite{luo2024incontext} and Plan-and-Solve Techniques\cite{wang2023planandsolve}.
    
    Another example of prompt strategies is Promptbreeder\cite{fernando2023promptbreeder}. In contrast to methods like CoT, which are based on hand-crafted prompts, Promptbreeder can generate optimized prompts. Promptbreeder relies on LLM and genetic algorithms to mutate a set of prompts, which are then given to the LLM. Strategies that use generated prompts have been shown to perform better than the prompt strategies that are based on hand-crafted prompts.

    The aforementioned strategies are more complex and require additional text to be added to the instruction. Simpler strategies are more oriented towards creating prompts that are clear and precise\cite{chen2023unleashing}. This includes forming unambiguous and specific prompts, allowing the model to give better and more detailed answers. Research has also shown that models can provide better answers when given emotional prompts\cite{li2023large}.
    
    %Matic: Sorry about the model references. They have so many authors. If it looks too weird I can remove them. I fix this by using et al -> Good! It looks more clear.
    Previously mentioned strategies have been tested on various LLMs. LLMs that are most commonly used for testing are chatGPT\cite{openai2024gpt4}, PaLM\cite{chowdhery2022palm}, Gemini\cite{geminiteam2023gemini} and other models. Most researchers prefer to use models with a large number of parameters because larger LLMs benefit more from prompt engineering. This has led to very little research being done on smaller models.
    
    %Matic I added some related work for datasets to related work 
    %Sienna: I checked references 7, 8, 9. Thank you for adding these. 
    Because of the growing number of prompting strategies, many datasets have been used to measure their performance. Examples of such datasets are the Winograd Schema Challenge\cite{levesque2012winograd}, Textbook Question Answering\cite{Kembhavi2017AreYS},  SocialIQA\cite{sap2019socialiqa}, and many other datasets. Many of the mentioned datasets were made to challenge LLM on commonsense sense reasoning tasks.

%------------------------------------------------
%Matic: this section colud be renamed to ideals and we can use it as a jumping off point at descrbing our goals for the project.
%Sienna: Please check the Ideals part. If you want to change the content, you can modify it.
%Matic: I think this part is very well done
\section*{Objectives}
In this project, our specific objectives are as follows:

\begin{enumerate}
    \item \textbf{Comparison of Various Prompt Strategies}:
    \begin{itemize}
        \item We aim to compare the performance of different prompt strategies through carefully designed experiments under specific conditions. The goal is to assess their effectiveness in various commonsense reasoning tasks quantitatively.
    \end{itemize}
    
    \item \textbf{Analysis of Commonsense Reasoning Processes}:
    \begin{itemize}
        \item Through meticulous analysis, we seek to understand the impact of each prompt strategy on the reasoning processes of the language models. This involves identifying how each strategy influences the decision-making process of the models, with the aim of enhancing their inference capabilities.
    \end{itemize}
    
    \item \textbf{Derivation of Optimal Prompt Strategies}:
    \begin{itemize}
        \item By analyzing the experimental results, we intend to derive optimal prompt strategies and propose the most effective approaches for commonsense reasoning tasks. Our objective is to maximize the performance of the models and enhance their applicability in real-world scenarios.
    \end{itemize}
    
    \item \textbf{Evaluation of Generalization Potential}:
    \begin{itemize}
        \item We aim to evaluate the potential of generalizing the findings of this project to different contexts and tasks. This assessment will help determine the broader applicability of our research across various domains.
    \end{itemize}
\end{enumerate}


%Sienna: Is it correct to write down the 'Initial idea' part here? And I think we have to talk about this part carefully because it is the main section of the first report. Since I only wrote comprehensive content, I think we need to talk specifically and make a clear plan.

%Matic: We could call it the initial plan since it describes the general layout of the project.

%Sienna: Matic, do we have to write more specific plan?

%Matic: I don't think so but I was also thinking about this yesterday. Usually, assistants don't require the plan to be very specific just the general plan and milestones (which we kind of have). 
\section*{Initial Idea}

\begin{enumerate}
    \item \textbf{Dataset Selection}:
    \begin{itemize}
        \item \textbf{Winograd Schema Challenge (WSC)}: WSC encompasses a spectrum of scenarios mimicking real-world situations, rendering it apt for assessing a model's common-sense reasoning prowess. We shall utilize WSC to scrutinize and juxtapose the common-sense reasoning proficiencies of various models.
        \item \textbf{Textbook Question Answering (TQA)}: TQA evaluates the adeptness to grasp textbook content and respond to associated queries. Employing this dataset will enable us to gauge whether a model can assimilate and apply the requisite common-sense knowledge to address real-world predicaments.
        \item \textbf{SocialIQA}: SocialIQA scrutinizes the acumen in social contexts, assessing common-sense communication aptitudes. This dataset will evaluate a model's capacity to discern common-sense decisions across diverse social scenarios.
    \end{itemize}
    
    \item \textbf{Prompt Strategies}:
    \begin{itemize}
        \item \textbf{Chain of Thought (CoT)}: The CoT strategy steers the model through sequential problem-solving processes or solicits step-by-step responses to enrich the model's common-sense reasoning acuity.
        \item \textbf{In-context Learning}: This strategy facilitates the model in assimilating novel information within a given context, empowering it to conduct superior reasoning by leveraging previously acquired knowledge.
        \item \textbf{Plan-and-Solve Techniques}: This strategy furnishes explicit blueprints and directs the model to resolve problems methodically. This is anticipated to elucidate the model's reasoning trajectory and refine its logical reasoning skills.
    \end{itemize}
    
    \item \textbf{Experiment Design and Analysis Plan}:
    \begin{itemize}
        \item Each experiment pertaining to both datasets and prompt strategies shall be meticulously delineated as follows:
            \begin{enumerate}
                \item \textbf{Dataset Selection and Preprocessing}:
                We shall leverage sentiment inference within three datasets. Each dataset will be segregated into training, validation, and test subsets, with judicious sampling techniques applied to mitigate data imbalances.
                
                \item \textbf{Implementation of Prompt Strategies}:
                Three prominent prompt strategies shall be implemented. Moreover, we shall tailor or extend these strategies for fine-tuning and devising novel approaches.
                
                \item \textbf{Experiment Design}:
                Experiments for each prompt strategy will adhere to a uniform structure. Each strategy will undergo training utilizing identical model architecture and hyperparameter settings. Experiments will be conducted under reproducible conditions to ensure replicability.
                
                \item \textbf{Performance Measurement}:
                We shall gauge the model's training and inference durations for each strategy. Performance metrics will encompass accuracy, precision, recall, and F1 scores across each dataset.
                
                \item \textbf{Experiment Execution and Analysis}:
                Experiments for each strategy will incorporate measures to ensure stability, such as cross-validation or bootstrapping. The resultant outcomes will be juxtaposed to dissect the performance of each strategy, with particular emphasis on discerning performance disparities in prevalent sentiment inference tasks.
                
                \item \textbf{Result Interpretation and Reporting}:
                Experiment findings will be expounded to discern the strengths and weaknesses of each strategy. Drawing upon this analysis, a conclusive report will be compiled, advocating the optimal prompt strategy and outlining avenues for future research.
            \end{enumerate}
    \end{itemize}
\end{enumerate}



\begin{comment}
Use the Methods section to describe what you did an how you did it -- in what way did you prepare the data, what algorithms did you use, how did you test various solutions ... Provide all the required details for a reproduction of your work.
%
%Below are \LaTeX examples of some common elements that you will probably need when writing your %report (e.g. figures, equations, lists, code examples ...).


\subsection*{Equations}

You can write equations inline, e.g. $\cos\pi=-1$, $E = m \cdot c^2$ and $\alpha$, or you can include them as separate objects. The Bayes’s rule is stated mathematically as:

\begin{equation}
	P(A|B) = \frac{P(B|A)P(A)}{P(B)},
	\label{eq:bayes}
\end{equation}

where $A$ and $B$ are some events. You can also reference it -- the equation \ref{eq:bayes} describes the Bayes's rule.

\subsection*{Lists}

We can insert numbered and bullet lists:

% the [noitemsep] option makes the list more compact
\begin{enumerate}[noitemsep] 
	\item First item in the list.
	\item Second item in the list.
	\item Third item in the list.
\end{enumerate}

\begin{itemize}[noitemsep] 
	\item First item in the list.
	\item Second item in the list.
	\item Third item in the list.
\end{itemize}

We can use the description environment to define or describe key terms and phrases.

\begin{description}
	\item[Word] What is a word?.
	\item[Concept] What is a concept?
	\item[Idea] What is an idea?
\end{description}


\subsection*{Random text}

This text is inserted only to make this template look more like a proper report. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam blandit dictum facilisis. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Interdum et malesuada fames ac ante ipsum primis in faucibus. Etiam convallis tellus velit, quis ornare ipsum aliquam id. Maecenas tempus mauris sit amet libero elementum eleifend. Nulla nunc orci, consectetur non consequat ac, consequat non nisl. Aenean vitae dui nec ex fringilla malesuada. Proin elit libero, faucibus eget neque quis, condimentum laoreet urna. Etiam at nunc quis felis pulvinar dignissim. Phasellus turpis turpis, vestibulum eget imperdiet in, molestie eget neque. Curabitur quis ante sed nunc varius dictum non quis nisl. Donec nec lobortis velit. Ut cursus, libero efficitur dictum imperdiet, odio mi fermentum dui, id vulputate metus velit sit amet risus. Nulla vel volutpat elit. Mauris ex erat, pulvinar ac accumsan sit amet, ultrices sit amet turpis.

Phasellus in ligula nunc. Vivamus sem lorem, malesuada sed pretium quis, varius convallis lectus. Quisque in risus nec lectus lobortis gravida non a sem. Quisque et vestibulum sem, vel mollis dolor. Nullam ante ex, scelerisque ac efficitur vel, rhoncus quis lectus. Pellentesque scelerisque efficitur purus in faucibus. Maecenas vestibulum vulputate nisl sed vestibulum. Nullam varius turpis in hendrerit posuere.


\subsection*{Figures}

You can insert figures that span over the whole page, or over just a single column. The first one, \figurename~\ref{fig:column}, is an example of a figure that spans only across one of the two columns in the report.

\begin{figure}[ht]\centering
	\includegraphics[width=\linewidth]{single_column.pdf}
	\caption{\textbf{A random visualization.} This is an example of a figure that spans only across one of the two columns.}
	\label{fig:column}
\end{figure}

On the other hand, \figurename~\ref{fig:whole} is an example of a figure that spans across the whole page (across both columns) of the report.

% \begin{figure*} makes the figure take up the entire width of the page
\begin{figure*}[ht]\centering 
	\includegraphics[width=\linewidth]{whole_page.pdf}
	\caption{\textbf{Visualization of a Bayesian hierarchical model.} This is an example of a figure that spans the whole width of the report.}
	\label{fig:whole}
\end{figure*}


\subsection*{Tables}

Use the table environment to insert tables.

\begin{table}[hbt]
	\caption{Table of grades.}
	\centering
	\begin{tabular}{l l | r}
		\toprule
		\multicolumn{2}{c}{Name} \\
		\cmidrule(r){1-2}
		First name & Last Name & Grade \\
		\midrule
		John & Doe & $7.5$ \\
		Jane & Doe & $10$ \\
		Mike & Smith & $8$ \\
		\bottomrule
	\end{tabular}
	\label{tab:label}
\end{table}


\subsection*{Code examples}

You can also insert short code examples. You can specify them manually, or insert a whole file with code. Please avoid inserting long code snippets, advisors will have access to your repositories and can take a look at your code there. If necessary, you can use this technique to insert code (or pseudo code) of short algorithms that are crucial for the understanding of the manuscript.

\lstset{language=Python}
\lstset{caption={Insert code directly from a file.}}
\lstset{label={lst:code_file}}
\lstinputlisting[language=Python]{code/example.py}

\lstset{language=R}
\lstset{caption={Write the code you want to insert.}}
\lstset{label={lst:code_direct}}
\begin{lstlisting}
import(dplyr)
import(ggplot)

ggplot(diamonds,
	   aes(x=carat, y=price, color=cut)) +
  geom_point() +
  geom_smooth()
\end{lstlisting}

%------------------------------------------------

\section*{Results}

Use the results section to present the final results of your work. Present the results in a objective and scientific fashion. Use visualisations to convey your results in a clear and efficient manner. When comparing results between various techniques use appropriate statistical methodology.

\subsection*{More random text}

This text is inserted only to make this template look more like a proper report. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam blandit dictum facilisis. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Interdum et malesuada fames ac ante ipsum primis in faucibus. Etiam convallis tellus velit, quis ornare ipsum aliquam id. Maecenas tempus mauris sit amet libero elementum eleifend. Nulla nunc orci, consectetur non consequat ac, consequat non nisl. Aenean vitae dui nec ex fringilla malesuada. Proin elit libero, faucibus eget neque quis, condimentum laoreet urna. Etiam at nunc quis felis pulvinar dignissim. Phasellus turpis turpis, vestibulum eget imperdiet in, molestie eget neque. Curabitur quis ante sed nunc varius dictum non quis nisl. Donec nec lobortis velit. Ut cursus, libero efficitur dictum imperdiet, odio mi fermentum dui, id vulputate metus velit sit amet risus. Nulla vel volutpat elit. Mauris ex erat, pulvinar ac accumsan sit amet, ultrices sit amet turpis.

Phasellus in ligula nunc. Vivamus sem lorem, malesuada sed pretium quis, varius convallis lectus. Quisque in risus nec lectus lobortis gravida non a sem. Quisque et vestibulum sem, vel mollis dolor. Nullam ante ex, scelerisque ac efficitur vel, rhoncus quis lectus. Pellentesque scelerisque efficitur purus in faucibus. Maecenas vestibulum vulputate nisl sed vestibulum. Nullam varius turpis in hendrerit posuere.

Nulla rhoncus tortor eget ipsum commodo lacinia sit amet eu urna. Cras maximus leo mauris, ac congue eros sollicitudin ac. Integer vel erat varius, scelerisque orci eu, tristique purus. Proin id leo quis ante pharetra suscipit et non magna. Morbi in volutpat erat. Vivamus sit amet libero eu lacus pulvinar pharetra sed at felis. Vivamus non nibh a orci viverra rhoncus sit amet ullamcorper sem. Ut nec tempor dui. Aliquam convallis vitae nisi ac volutpat. Nam accumsan, erat eget faucibus commodo, ligula dui cursus nisi, at laoreet odio augue id eros. Curabitur quis tellus eget nunc ornare auctor.


%------------------------------------------------

\section*{Discussion}

Use the Discussion section to objectively evaluate your work, do not just put praise on everything you did, be critical and exposes flaws and weaknesses of your solution. You can also explain what you would do differently if you would be able to start again and what upgrades could be done on the project in the future.


%------------------------------------------------

\section*{Acknowledgments}

Here you can thank other persons (advisors, colleagues ...) that contributed to the successful completion of your project.

\end{comment}
%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{report}


\end{document}